import streamlit as st
import pandas as pd
import requests
import re
import os
from bs4 import BeautifulSoup

FDA_URL = "https://www.fda.gov/drugs/drug-safety-and-availability/drug-safety-communications"

# ---------- è³‡æ–™æ¸…ç†èˆ‡æ­£è¦åŒ– ----------
def clean_text(s):
    return re.sub(r"\s+", " ", str(s)).strip().lower()

def normalize_brand(s):
    s = clean_text(s)
    s = re.sub(r"\b(tablets?|capsules?|injection|solution|concentrate|for infusion|pre-filled syringe)\b", "", s)
    return re.sub(r"[^\w\s]", "", s).strip()

SYNONYMS = {
    "hyoscine": "scopolamine",
    "scopolamine butylbromide": "scopolamine",
    "hyoscine n butylbromide": "scopolamine",
    "glatiramer": "glatiramer acetate",
    "lecanemab": "lecanemab",
    "clozapine": "clozapine",
    "cetirizine": "cetirizine",
    "levocetirizine": "levocetirizine",
    "methylphenidate": "methylphenidate",
    "amphetamine": "amphetamine",
}

def normalize_ingredient_token(tok):
    tok = clean_text(tok)
    for salt in ["hbr", "bromide", "acetate", "tartrate", "hcl", "maleate", "methylsulfate"]:
        tok = tok.replace(salt, "")
    tok = re.sub(r"[^\w\s]", "", tok).strip()
    return SYNONYMS.get(tok, tok)

def split_ingredients(s):
    parts = re.split(r";|,|/| and |\+|\|", str(s))
    return list({normalize_ingredient_token(p) for p in parts if p.strip()})

# ---------- FDA æŠ“å–èˆ‡è§£æ ----------
def fetch_html(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    r = requests.get(url, headers=headers, timeout=30)
    r.raise_for_status()
    return r.text

def parse_current_list(html):
    soup = BeautifulSoup(html, "html.parser")
    header = soup.find(lambda tag: tag.name in ["h2", "h3"] and "Current Drug Safety Communications" in tag.text)
    items = []
    if header:
        ul = header.find_next("ul")
        for li in ul.find_all("li"):
            a = li.find("a", href=True)
            if not a:
                continue
            li_text = li.get_text(" ", strip=True)
            m_date = re.match(r"^\s*(\d{2}-\d{2}-\d{4})\b", li_text)
            date = m_date.group(1) if m_date else ""
            title = a.get("title") or a.get_text(strip=True)
            href = a["href"]
            if href.startswith("/"):
                href = "https://www.fda.gov" + href
            items.append({"date": date, "title": title, "href": href})
    return items

EXCLUDE_TOKENS = {"rems", "program", "strategy", "risk", "evaluation", "mitigation", "warning", "labeling"}
KNOWN_INGREDIENTS = set(SYNONYMS.keys())

def extract_fields(title):
    t = title.lower()
    paren = re.findall(r"\(([^)]+)\)", t)
    if paren:
        ing_raw = paren[-1]
        ing_list = split_ingredients(ing_raw)
        ing_list = [tok for tok in ing_list if tok not in EXCLUDE_TOKENS]
    else:
        tokens = re.findall(r"[a-zA-Z][a-zA-Z\-]+", t)
        norm_tokens = [normalize_ingredient_token(x) for x in tokens]
        ing_list = sorted({tok for tok in norm_tokens if tok in KNOWN_INGREDIENTS and tok not in EXCLUDE_TOKENS})
    if paren:
        product = title.split("(")[0].strip()
    else:
        product = ""
        for k in KNOWN_INGREDIENTS:
            if re.search(rf"\b{k}\b", t):
                m = re.search(rf"\b({k})\b", t)
                if m:
                    product = m.group(1)
                    break
        if not product:
            product = title.split(":")[0].split("â€“")[0].split("-")[0].strip()
    return {"product": product, "ingredient_list": ing_list}

def build_fda_df(items):
    if not items:
        return pd.DataFrame(columns=["æ—¥æœŸ","å“å","ä¸»æˆåˆ†","å®‰å…¨è­°é¡Œ","ç”¨è—¥æ—ç¾¤","æ³¨æ„äº‹é …èˆ‡å°ç­–","source_title","source_url"])
    rows = []
    for it in items:
        fields = extract_fields(it["title"])
        rows.append({
            "æ—¥æœŸ": it["date"],
            "å“å": fields["product"],
            "ä¸»æˆåˆ†": ", ".join(fields["ingredient_list"]),
            "å®‰å…¨è­°é¡Œ": "",
            "ç”¨è—¥æ—ç¾¤": "",
            "æ³¨æ„äº‹é …èˆ‡å°ç­–": "",
            "source_title": it["title"],
            "source_url": it["href"]
        })
    return pd.DataFrame(rows)

def fallback_seed():
    return pd.DataFrame([
        {"æ—¥æœŸ":"2025-08-28","å“å":"Leqembi","ä¸»æˆåˆ†":"lecanemab","å®‰å…¨è­°é¡Œ":"å»ºè­°æ›´æ—© MRI ç›£æ¸¬","ç”¨è—¥æ—ç¾¤":"é˜¿èŒ²æµ·é»˜ç—‡æ‚£è€…","æ³¨æ„äº‹é …èˆ‡å°ç­–":"èª¿æ•´ MRI é »ç‡","source_title":"Leqembi (lecanemab)","source_url":"https://www.fda.gov/drugs/drug-safety-and-availability/fda-recommend-additional-earlier-mri-monitoring-patients-alzheimers-disease-taking-leqembi-lecanemab"},
        {"æ—¥æœŸ":"2025-08-27","å“å":"Clozapine","ä¸»æˆåˆ†":"clozapine","å®‰å…¨è­°é¡Œ":"ç§»é™¤ REMS è¨ˆç•«","ç”¨è—¥æ—ç¾¤":"ç²¾ç¥åˆ†è£‚ç—‡æ‚£è€…","æ³¨æ„äº‹é …èˆ‡å°ç­–":"ä¾æ–°æ¨™ç¤ºèª¿æ•´ç›£æ¸¬","source_title":"Clozapine","source_url":"https://www.fda.gov/drugs/drug-safety-and-availability/fda-removes-risk-evaluation-and-mitigation-strategy-rems-program-antipsychotic-drug-clozapine"},
        {"æ—¥æœŸ":"2025-01-22","å“å":"Copaxone","ä¸»æˆåˆ†":"glatiramer acetate","å®‰å…¨è­°é¡Œ":"éæ•æ€§ä¼‘å…‹è­¦ç¤º","ç”¨è—¥æ—ç¾¤":"å¤šç™¼æ€§ç¡¬åŒ–ç—‡æ‚£è€…","æ³¨æ„äº‹é …èˆ‡å°ç­–":"å‡ºç¾éæ•å¾µå…†ç«‹å³åœè—¥","source_title":"Copaxone (glatiramer acetate)","source_url":"https://www.fda.gov/drugs/drug-safety-and-availability/fda-adds-boxed-warning-about-rare-serious-allergic-reaction-called-anaphylaxis-multiple-sclerosis"},
        {"æ—¥æœŸ":"2025-06-18","å“å":"Transderm ScÅp","ä¸»æˆåˆ†":"scopolamine","å®‰å…¨è­°é¡Œ":"é«˜æº«ä½µç™¼ç—‡é¢¨éšª","ç”¨è—¥æ—ç¾¤":"ä½¿ç”¨æŠ—æšˆè²¼ç‰‡è€…","æ³¨æ„äº‹é …èˆ‡å°ç­–":"é«˜æº«ç’°å¢ƒæ…ç”¨","source_title":"Transderm ScÅp (scopolamine)","source_url":"https://www.fda.gov/drugs/drug-safety-and-availability/fda-adds-warning-about-serious-risk-heat-related-complications-antinausea-patch-transderm-scop"},
    ])

@st.cache_data
def load_tw_data():
    path = os.path.join(os.path.dirname(__file__), "37_2c.csv")
    df = pd.read_csv(path)
    df["tw_e_brand_norm"] = df["tw_e_brand"].apply(normalize_brand)
    df["tw_ing_list"] = df["tw_ingredient"].apply(split_ingredients)
    return df

def match_tw_products(fda_df, tw_df):
    matches = []
    for _, row in fda_df.iterrows():
        fda_brand = normalize_brand(row["å“å"])

        # åªåšå“ç‰Œæ¯”å°
        brand_hits = tw_df[tw_df["tw_e_brand_norm"] == fda_brand]
        for _, tw in brand_hits.iterrows():
            matches.append({
                "æ—¥æœŸ": row["æ—¥æœŸ"],
                "FDA_å“å": row["å“å"],
                "FDA_ä¸»æˆåˆ†": row["ä¸»æˆåˆ†"],
                "è—¥è­‰è™Ÿç¢¼": tw["tw_id"],
                "ä¸­æ–‡å“å": tw["tw_c_brand"],
                "è‹±æ–‡å“å": tw["tw_e_brand"],
                "åŠ‘å‹": tw["tw_form"],
                "ä¸»æˆåˆ†": tw["tw_ingredient"],
                "è—¥å•†": tw["tw_company"],
                "æ¯”å°æ–¹å¼": "å“ç‰Œå‘½ä¸­"
            })
    return pd.DataFrame(matches)

# ---------- Streamlit UI ----------

st.set_page_config(page_title="FDA é€šå ±è§£æèˆ‡å°ç£å“é …æ¯”å°", layout="wide")
st.title("ğŸ’Š FDA è—¥å“å®‰å…¨é€šå ±è§£æèˆ‡å°ç£å“é …æ¯”å°")

# æŠ“å– FDA é€šå ±
st.info("æ­£åœ¨æŠ“å– FDA é€šå ±è³‡æ–™â€¦")
try:
    html = fetch_html(FDA_URL)
    items = parse_current_list(html)
    fda_df = build_fda_df(items)
    if fda_df.empty:
        st.warning("âš ï¸ FDA HTML è§£æå¤±æ•—ï¼Œå·²è¼‰å…¥ç¨®å­è³‡æ–™ã€‚")
        fda_df = fallback_seed()
    else:
        st.success(f"å·²è§£æ FDA é€šå ± {len(fda_df)} ç­†")
except Exception as e:
    st.error(f"FDA ç¶²é æŠ“å–å¤±æ•—ï¼š{e}")
    fda_df = fallback_seed()

# é¡¯ç¤º FDA é€šå ±è¡¨æ ¼
st.subheader("FDA Current Drug Safety Communications")
fda_df_display = fda_df.copy()
fda_df_display["åŸå§‹é€šå ±"] = fda_df_display.apply(
    lambda r: f'<a href="{r["source_url"]}" target="_blank">é€£çµ</a>', axis=1
)
st.write(
    fda_df_display[["æ—¥æœŸ","å“å","ä¸»æˆåˆ†","åŸå§‹é€šå ±"]].to_html(escape=False, index=False),
    unsafe_allow_html=True
)

# è¼‰å…¥å°ç£å“é …
st.info("æ­£åœ¨è¼‰å…¥å°ç£å“é …è³‡æ–™â€¦")
try:
    tw_df = load_tw_data()
    st.success(f"å·²è¼‰å…¥å°ç£å“é … {len(tw_df)} ç­†")
except Exception as e:
    st.error(f"CSV è¼‰å…¥å¤±æ•—ï¼š{e}")
    tw_df = pd.DataFrame()

# æ¯”å°é‚è¼¯å‘ˆç¾
if not fda_df.empty and not tw_df.empty:
    # å¯èƒ½ç›¸é—œå°ç£å“é …ï¼ˆä¸»æˆåˆ†äº¤é›†ï¼‰
    relevant_tokens = set()
    for ing in fda_df["ä¸»æˆåˆ†"].dropna():
        relevant_tokens.update(split_ingredients(ing))
    cand_tw = tw_df[tw_df["tw_ing_list"].apply(lambda lst: bool(set(lst) & relevant_tokens))]
    st.subheader(f"ğŸ” å¯èƒ½ç›¸é—œå°ç£å“é …ï¼ˆ{len(cand_tw)} ç­†ï¼‰")
    st.dataframe(cand_tw[
        ["tw_id","tw_c_brand","tw_e_brand","tw_form","tw_ingredient","tw_company"]
    ], use_container_width=True)

    # ç‰¹åˆ¥æŒ‘å‡ºè—¥å•†ç‚ºã€Œä¸­åœ‹åŒ–å­¸ã€æˆ–ã€Œä¸­åŒ–è£•æ°‘ã€
    special_tw = cand_tw[cand_tw["tw_company"].str.contains("ä¸­åœ‹åŒ–å­¸|ä¸­åŒ–è£•æ°‘", na=False)]
    if not special_tw.empty:
        st.subheader(f"â­ ç‰¹åˆ¥é—œæ³¨è—¥å•†ï¼ˆä¸­åœ‹åŒ–å­¸ / ä¸­åŒ–è£•æ°‘ï¼‰ç›¸é—œå“é …ï¼ˆ{len(special_tw)} ç­†ï¼‰")
        st.dataframe(special_tw[
            ["tw_id","tw_c_brand","tw_e_brand","tw_form","tw_ingredient","tw_company"]
        ], use_container_width=True)
